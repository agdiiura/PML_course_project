---
title: "Course Project"
author: "Andrea Di Iura"
#date: "30 novembre 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Course Project

### Load data

We load data from the directory `data`.

```{r}
training_name = "data/pml-training.csv"
testing_name = "data/pml-testing.csv"

training = read.csv(training_name, na.strings=c("", "NA", "#DIV/0!"))
testing = read.csv(testing_name, na.strings=c("", "NA", "#DIV/0!"))
```

Using the commands `head`, `str` and `summary` we investigate the training dataset, the output is quite long thus we do not show it here. The datasets have dimensions 

```{r}
dim(training)
dim(testing)
```

We can reduce the number of columns in the training dataset because the first column contains the indeces. We also remove the columns with no NA values and near zero variance variables using the function `nearZeroVar` contained in the `caret` library. Also the variables $1\div6$ can be neglected. To do this we create a mask variable called `mask_training`.

```{r}
training$X = NULL
testing$X = NULL
```

```{r}
library(caret)
NZV = nearZeroVar(training, saveMetrics= TRUE)
mask_training = !(colSums(is.na(training)) > 0) & !NZV$nzv

training_small = training[,mask_training]
testing_small = testing[,mask_training]

training_small = training_small[,-c(1:6)]
testing_small = testing_small[,-c(1:6)]
```

We reduce the dimensions
```{r}
dim(training_small)
dim(testing_small)
```

The training and the validation datasets can be created using the `caret` function `createDataPartition`. For reproducability we also set seed


```{r}
set.seed(1234)


inTrain = createDataPartition(y = training_small$classe, p = 0.05, list=FALSE)
train = training_small[inTrain,]
valid = training_small[-inTrain,]
```
```{r}
dim(train)
dim(valid)
```

## Exploratory analysis

The first step is to do some exploratory analysis to explore possible correlations among the variables. A first step is to use the package `corrplot` to see all the possible correlations in the dataset.


```{r, fig.width=10, fig.height=10}
library(corrplot)

MCorr = cor(train[,-dim(train)[2]])
corrplot(MCorr, cl.ratio=0.2, cl.align="r")
```

No variables with very low variance are found in `train` dataset. We could omit highly correlated variables, over correlation of 0.75

```{r}
HighCorr =  findCorrelation(MCorr, cutoff =  0.75)
# Subset data with our correlation limit
train = train[-HighCorr]
valid = valid[-HighCorr]

testing_small = testing_small[-HighCorr]
```
Thus we can reduce the dimensions.
```{r}
dim(train)
dim(valid)
```

## Machine Learning Analysis

To construct an accurate model we combine 3 different alghoritms: Supported Vector Machines (SVM), Linear Discriminant Analysis (LDA) and Random Forest (RF). With the `caret` package we evaluate the models on the `train` dataset

```{r}
control_method  =  trainControl(method = 'cv', number =  5)

mod_svm = train(classe ~ ., data = train, method = "svmLinear", trControl = control_method, verbose = FALSE)
mod_lda = train(classe ~ ., data = train, method = "lda", verbose = FALSE)
mod_rf = train(classe ~ ., data = train, method = "rf", trControl = control_method, verbose = FALSE)
```
Here the confusion matrices for the models considered above using the `valid` datasets.
```{r}
print("############")
print("SVM - method")
print("############")
prediction_svm = predict(mod_svm, valid)
confusionMatrix(prediction_svm, valid$classe)

print("############")
print("LDA - method")
print("############")
prediction_lda = predict(mod_lda, valid)
confusionMatrix(prediction_lda, valid$classe)

print("###########")
print("RF - method")
print("###########")
prediction_rf = predict(mod_rf, valid)
confusionMatrix(prediction_rf, valid$classe)
```
We can create a combined dataframe to improve our predictions. We use a Random Forest method to construct the final model. 
```{r}
combined_df = data.frame(prediction_svm, prediction_lda, prediction_rf, classe = valid$classe)
model_combined = train(classe ~ ., method = "rf", data = combined_df, verbose = FALSE)
```

The results of the combined fit are, where we observe a better result with respect to the Random Forest model
```{r}
prediction_combined = predict(model_combined, combined_df)
confusionMatrix(prediction_combined, combined_df$classe)
```


## Testing evaluation

We can compute the output using the `training` dataset. The prediction for `testing_small` are
```{r}

prediction_svm = predict(mod_svm, testing_small)
prediction_lda = predict(mod_lda, testing_small)
prediction_rf = predict(mod_rf, testing_small)

testing_df = data.frame(prediction_svm, prediction_lda, prediction_rf)

testing_prediction = predict(model_combined, newdata = testing_df)
testing_prediction
```

The submission to Coursera can be done using the following script
```{r}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}

pml_write_files(testing_prediction)

```

